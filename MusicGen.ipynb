{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ylacombe/musicgen-dreamboothing/blob/main/MusicGen_Dreamboothing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "NzASScTyfWW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3.12"
      ],
      "metadata": {
        "id": "dzwjreSrY_Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oj7rwoBZ-u5"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet git+https://github.com/ylacombe/musicgen-dreamboothing demucs msclap\n",
        "!pip install -U git+https://github.com/huggingface/transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJCHB-SC9Ysh"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"XXX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22EQBkzYc0Be"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yav1327/indian_songs\", split='train[:30]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G2hQh2g9Ysh"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iysjug_Hfvjn"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(dataset[\"filepath\"][0][\"array\"], rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sbWxb59fo1m"
      },
      "outputs": [],
      "source": [
        "from demucs import pretrained\n",
        "from demucs.apply import apply_model\n",
        "from demucs.audio import convert_audio\n",
        "from datasets import Audio\n",
        "import torch\n",
        "\n",
        "demucs = pretrained.get_model(\"htdemucs\")\n",
        "if torch.cuda.device_count() > 0:\n",
        "    demucs.to(\"cuda:0\")\n",
        "\n",
        "audio_column_name = \"audio_files\"\n",
        "\n",
        "def wrap_audio(audio, sr):\n",
        "    return {\"array\": audio.cpu().numpy(), \"sampling_rate\": sr}\n",
        "\n",
        "def filter_stems(batch, rank=None):\n",
        "    device = \"cpu\" if torch.cuda.device_count() == 0 else \"cuda:0\"\n",
        "\n",
        "    wavs = [\n",
        "        convert_audio(\n",
        "            torch.tensor(audio[\"array\"][None], device=device).to(\n",
        "                torch.float32\n",
        "            ),\n",
        "            44100,\n",
        "            demucs.samplerate,\n",
        "            demucs.audio_channels,\n",
        "        ).T\n",
        "        for audio in batch[\"filepath\"]\n",
        "    ]\n",
        "    wavs_length = [audio.shape[0] for audio in wavs]\n",
        "\n",
        "    wavs = torch.nn.utils.rnn.pad_sequence(\n",
        "        wavs, batch_first=True, padding_value=0.0\n",
        "    ).transpose(1, 2)\n",
        "    stems = apply_model(demucs, wavs)\n",
        "\n",
        "    batch[audio_column_name] = [\n",
        "        wrap_audio(s[:-1, :, :length].sum(0).mean(0), demucs.samplerate)\n",
        "        for (s, length) in zip(stems, wavs_length)\n",
        "    ]\n",
        "\n",
        "    return batch\n",
        "\n",
        "num_proc = 1\n",
        "\n",
        "dataset = dataset.map(\n",
        "    filter_stems,\n",
        "    batched=True,\n",
        "    batch_size=8,\n",
        "    with_rank=True,\n",
        "    num_proc=num_proc,\n",
        ")\n",
        "dataset = dataset.cast_column(audio_column_name, Audio())\n",
        "\n",
        "del demucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNOGlnLdfobv"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(dataset[0][\"filepath\"][\"array\"], rate=dataset[0][\"filepath\"][\"sampling_rate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GMg-fqOkIP2"
      },
      "outputs": [],
      "source": [
        "from utils import instrument_classes, genre_labels, mood_theme_classes\n",
        "print(\"Genres\", genre_labels)\n",
        "print(\"Instruments:\", instrument_classes)\n",
        "print(\"Moods\", mood_theme_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt0yKfjei4gx"
      },
      "outputs": [],
      "source": [
        "from msclap import CLAP\n",
        "import librosa\n",
        "import tempfile\n",
        "import torchaudio\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "clap_model = CLAP(version=\"2023\", use_cuda=True)\n",
        "instrument_embeddings = clap_model.get_text_embeddings(instrument_classes)\n",
        "genre_embeddings = clap_model.get_text_embeddings(genre_labels)\n",
        "mood_embeddings = clap_model.get_text_embeddings(mood_theme_classes)\n",
        "\n",
        "def enrich_text(batch):\n",
        "    audio, sampling_rate = (\n",
        "        batch[\"filepath\"][\"array\"],\n",
        "        batch[\"filepath\"][\"sampling_rate\"],\n",
        "    )\n",
        "\n",
        "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sampling_rate)\n",
        "    tempo = f\"{str(np.round(tempo))} bpm\"\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sampling_rate)\n",
        "    key = np.argmax(np.sum(chroma, axis=1))\n",
        "    key = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"][key]\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tempdir:\n",
        "        path = os.path.join(tempdir, \"tmp.wav\")\n",
        "        torchaudio.save(path, torch.tensor(audio).unsqueeze(0), sampling_rate)\n",
        "        audio_embeddings = clap_model.get_audio_embeddings([path])\n",
        "\n",
        "    instrument = clap_model.compute_similarity(\n",
        "        audio_embeddings, instrument_embeddings\n",
        "    ).argmax(dim=1)[0]\n",
        "    genre = clap_model.compute_similarity(\n",
        "        audio_embeddings, genre_embeddings\n",
        "    ).argmax(dim=1)[0]\n",
        "    mood = clap_model.compute_similarity(\n",
        "        audio_embeddings, mood_embeddings\n",
        "    ).argmax(dim=1)[0]\n",
        "\n",
        "    instrument = instrument_classes[instrument]\n",
        "    genre = genre_labels[genre]\n",
        "    mood = mood_theme_classes[mood]\n",
        "\n",
        "    metadata = [key, tempo, instrument, genre, mood]\n",
        "\n",
        "    random.shuffle(metadata)\n",
        "    batch[\"metadata\"] = \", \".join(metadata)\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(\n",
        "    enrich_text,\n",
        "    desc=\"add metadata\",\n",
        ")\n",
        "\n",
        "del clap_model, instrument_embeddings, genre_embeddings, mood_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsymwWdMzSHV"
      },
      "outputs": [],
      "source": [
        "print(dataset[0][\"metadata\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mJMcHi4y8vr"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForTextToWaveform,\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-melody\")\n",
        "model = AutoModelForTextToWaveform.from_pretrained(\"facebook/musicgen-melody\")\n",
        "\n",
        "model.freeze_text_encoder()\n",
        "model.freeze_audio_encoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWzYXGB3Cvr9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "inputs = processor(\n",
        "    text=[\"Generate bollywood sad song\"],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(device)\n",
        "\n",
        "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
        "\n",
        "\n",
        "Audio(audio_values.cpu().numpy().squeeze(), rate=32000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQhHi1IG1ehY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "from datasets import Audio\n",
        "\n",
        "instance_prompt = \"punk\"\n",
        "\n",
        "# take audio_encoder_feature_extractor\n",
        "audio_encoder_feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "    model.config.audio_encoder._name_or_path,\n",
        ")\n",
        "\n",
        "# resample audio if necessary\n",
        "dataset_sampling_rate = dataset[0][\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "if dataset_sampling_rate != audio_encoder_feature_extractor.sampling_rate:\n",
        "    dataset = dataset.cast_column(\n",
        "        \"audio\",\n",
        "        Audio(\n",
        "            sampling_rate=audio_encoder_feature_extractor.sampling_rate\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "# We need to read the audio files as arrays and tokenize the targets.\n",
        "def prepare_audio_features(batch):\n",
        "    # load audio\n",
        "\n",
        "    metadata = batch[\"metadata\"]\n",
        "    metadata = f\"{instance_prompt}, {metadata}\"\n",
        "    batch[\"input_ids\"] = processor.tokenizer(metadata)[\"input_ids\"]\n",
        "\n",
        "    # load audio\n",
        "    target_sample = batch[\"audio\"]\n",
        "    labels = audio_encoder_feature_extractor(\n",
        "        target_sample[\"array\"], sampling_rate=target_sample[\"sampling_rate\"]\n",
        "    )\n",
        "    batch[\"labels\"] = labels[\"input_values\"]\n",
        "\n",
        "    # take length of raw audio waveform\n",
        "    batch[\"target_length\"] = len(target_sample[\"array\"].squeeze())\n",
        "    return batch\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    prepare_audio_features,\n",
        "    remove_columns=dataset.column_names,\n",
        "    num_proc=2,\n",
        "    desc=\"preprocess datasets\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i6jp3QQ3drB"
      },
      "outputs": [],
      "source": [
        "\n",
        "audio_decoder = model.audio_encoder\n",
        "num_codebooks = model.decoder.config.num_codebooks\n",
        "audio_encoder_pad_token_id = model.config.decoder.pad_token_id\n",
        "\n",
        "pad_labels = torch.ones((1, 1, num_codebooks, 1)) * audio_encoder_pad_token_id\n",
        "\n",
        "if torch.cuda.device_count() == 1:\n",
        "    audio_decoder.to(\"cuda\")\n",
        "\n",
        "def apply_audio_decoder(batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        labels = audio_decoder.encode(\n",
        "            torch.tensor(batch[\"labels\"]).to(audio_decoder.device)\n",
        "        )[\"audio_codes\"]\n",
        "\n",
        "    # add pad token column\n",
        "    labels = torch.cat(\n",
        "        [pad_labels.to(labels.device).to(labels.dtype), labels], dim=-1\n",
        "    )\n",
        "\n",
        "    labels, delay_pattern_mask = model.decoder.build_delay_pattern_mask(\n",
        "        labels.squeeze(0),\n",
        "        audio_encoder_pad_token_id,\n",
        "        labels.shape[-1] + num_codebooks,\n",
        "    )\n",
        "\n",
        "    labels = model.decoder.apply_delay_pattern_mask(labels, delay_pattern_mask)\n",
        "\n",
        "    # the first timestamp is associated to a row full of BOS, let's get rid of it\n",
        "    batch[\"labels\"] = labels[:, 1:].cpu()\n",
        "    return batch\n",
        "\n",
        "# Encodec doesn't truely support batching\n",
        "# Pass samples one by one to the GPU\n",
        "dataset = dataset.map(\n",
        "    apply_audio_decoder,\n",
        "    num_proc=1,\n",
        "    desc=\"Apply encodec\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSP44t4U08EV"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# TODO(YL): add modularity here\n",
        "target_modules = (\n",
        "    [\n",
        "        \"enc_to_dec_proj\",\n",
        "        \"audio_enc_to_dec_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"q_proj\",\n",
        "        \"out_proj\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "        \"lm_heads.0\",\n",
        "    ]\n",
        "    + [f\"lm_heads.{str(i)}\" for i in range(len(model.decoder.lm_heads))]\n",
        "    + [f\"embed_tokens.{str(i)}\" for i in range(len(model.decoder.lm_heads))]\n",
        ")\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjmDJwEajZWi"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from dataclasses import dataclass\n",
        "\n",
        "class MusicgenTrainer(Seq2SeqTrainer):\n",
        "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
        "        if self.tokenizer is not None and hasattr(self.tokenizer, \"pad_token_id\"):\n",
        "            # If PAD token is not defined at least EOS token has to be defined\n",
        "            pad_token_id = (\n",
        "                self.tokenizer.pad_token_id\n",
        "                if self.tokenizer.pad_token_id is not None\n",
        "                else self.tokenizer.eos_token_id\n",
        "            )\n",
        "        else:\n",
        "            if self.model.config.pad_token_id is not None:\n",
        "                pad_token_id = self.model.config.pad_token_id\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Pad_token_id must be set in the configuration of the model, in order to pad tensors\"\n",
        "                )\n",
        "\n",
        "        padded_tensor = pad_token_id * torch.ones(\n",
        "            (tensor.shape[0], max_length, tensor.shape[2]),\n",
        "            dtype=tensor.dtype,\n",
        "            device=tensor.device,\n",
        "        )\n",
        "        length = min(max_length, tensor.shape[1])\n",
        "        padded_tensor[:, :length] = tensor[:, :length]\n",
        "        return padded_tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorMusicGenWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.AutoProcessor`)\n",
        "            The processor used for proccessing the data.\n",
        "    \"\"\"\n",
        "\n",
        "    processor: AutoProcessor\n",
        "\n",
        "    def __call__(\n",
        "        self, features\n",
        "    ):\n",
        "        # split inputs and labels since they have to be of different lengths and need\n",
        "        # different padding methods\n",
        "        labels = [\n",
        "            torch.tensor(feature[\"labels\"]).transpose(0, 1) for feature in features\n",
        "        ]\n",
        "        # (bsz, seq_len, num_codebooks)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(\n",
        "            labels, batch_first=True, padding_value=-100\n",
        "        )\n",
        "\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        input_ids = self.processor.tokenizer.pad(input_ids, return_tensors=\"pt\")\n",
        "\n",
        "        batch = {\"labels\": labels, **input_ids}\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Instantiate custom data collator\n",
        "data_collator = DataCollatorMusicGenWithPadding(\n",
        "    processor=processor,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-BOYKBJ4onG"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "      output_dir=\"./output/\",\n",
        "      num_train_epochs=4,\n",
        "      gradient_accumulation_steps=8,\n",
        "      gradient_checkpointing=True,\n",
        "      per_device_train_batch_size= 2,\n",
        "      learning_rate=2e-4,\n",
        "      weight_decay=0.1,\n",
        "      adam_beta2=0.99,\n",
        "      fp16=True,\n",
        "      dataloader_num_workers=2,\n",
        "      logging_steps=2,\n",
        "      report_to=\"none\",\n",
        "      push_to_hub=True,\n",
        "      push_to_hub_model_id=\"music-gen-indian-songs\",\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize MusicgenTrainer\n",
        "trainer = MusicgenTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.save_state()\n",
        "\n",
        "\n",
        "kwargs = {\n",
        "    \"finetuned_from\": \"facebook/musicgen-melody\",\n",
        "    \"tasks\": \"text-to-audio\",\n",
        "    \"tags\": [\"text-to-audio\", \"tiny-punk\"],\n",
        "    \"dataset\": \"yav1327/indian_songs\",\n",
        "}\n",
        "\n",
        "trainer.push_to_hub(**kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZNI5gDHCAGr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForTextToWaveform, AutoProcessor\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.device_count()>0 else \"cpu\")\n",
        "\n",
        "repo_id = \"yav1327/musicgen-melody-lora-punk-colab\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(repo_id)\n",
        "model = AutoModelForTextToWaveform.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, repo_id).to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(repo_id)\n",
        "\n",
        "inputs = processor(\n",
        "    text=[\"Generate bollywood sad song\"],\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(device)\n",
        "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxynM8o5_A3R"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(audio_values.cpu().numpy().squeeze(), rate=44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paqCStiRVKfM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}